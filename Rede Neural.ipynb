{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Funções de Ativação](#Funções-de-Ativação)\n",
    "\n",
    "[Implementação](#Implementação)\n",
    "\n",
    "[Teste](#Teste)\n",
    "\n",
    "[Referências](#Referências)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de Ativação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(x, derivative=False):\n",
    "    return np.ones_like(x) if derivative else x\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "    if derivative:\n",
    "        y = sigmoid(x)\n",
    "        return y*(1-y)\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "    if derivative:\n",
    "        y = tanh(x)\n",
    "        return 1 - y**2\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return np.where(x <= 0, 0, 1)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return np.where(x <= 0, 0.1, 1)\n",
    "    return np.where(x < 0, 0.1*x, x)\n",
    "\n",
    "def gaussian(x, derivative=False):\n",
    "    if derivative:\n",
    "        return -2*x*np.exp(-x**2)\n",
    "    return np.exp(-x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de custo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mse(y, y_pred, derivative=False):\n",
    "    if derivative:\n",
    "        return np.mean(-(y - y_pred))\n",
    "    return np.mean((y - y_pred)**2)\n",
    "\n",
    "def sigmoid_cross_entropy(y, y_pred, derivative=False):\n",
    "    if derivative:\n",
    "        pass\n",
    "    return -np.mean(y*np.log(y_pred) + (1-y)*np.log(1-y_pred))\n",
    "\n",
    "def softmax_cross_entropy(y, y_pred, derivative=False):\n",
    "    if derivative:\n",
    "        return -( y*(1/y_pred) + (1-y)*(1/(1-y_pred)) )\n",
    "    return -np.mean((y*np.log(y_pred) + (1-y)*np.log(1-y_pred)).sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, layers_size, activations, cost_func, learning_rate=1e-3):\n",
    "        self.layers_size = layers_size\n",
    "        self.activations = activations\n",
    "        self.cost_func = cost_func\n",
    "        self.learning_rate = learning_rate\n",
    "        self._layers_inp = [] # entrada da função de ativação\n",
    "        self._layers_out = []\n",
    "        self.weights, self.biases = self.__init_weights_and_biases()\n",
    "            \n",
    "    def fit(self, x, y, epochs=100, verbose=10):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.__feedforward(x)\n",
    "            self.__backprop(y, y_pred)\n",
    "            \n",
    "            if epoch % verbose == 0:\n",
    "                cost = self.cost_func(y, y_pred)\n",
    "                print(\"epoch: {0:=4}/{1} cost: {2}\".format(epoch, epochs, cost))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.__feedforward(x)\n",
    "    \n",
    "    def __init_weights_and_biases(self):\n",
    "        weights, biases = [], []\n",
    "        for inp_layer, out_layer in zip(self.layers_size[:-1], self.layers_size[1:]):\n",
    "            weights.append(np.random.randn(out_layer, inp_layer))\n",
    "            biases.append(np.random.randn(1, out_layer))\n",
    "        return weights, biases\n",
    "        \n",
    "    def __feedforward(self, x):\n",
    "        self._layers_inp, self._layers_out = [], []\n",
    "        self._layers_out.append(x)\n",
    "        for w, b, activation in zip(self.weights, self.biases, self.activations):\n",
    "            y = np.dot(self._layers_out[-1], w.T) + b\n",
    "            self._layers_inp.append(y)\n",
    "            self._layers_out.append(activation(y))\n",
    "        return self._layers_out[-1]\n",
    "    \n",
    "    def __backprop(self, y, y_pred):\n",
    "        self._layers_out.pop()\n",
    "        \n",
    "        last_delta = self.cost_func(y, y_pred, derivative=True)\n",
    "        dweights, dbiases = [], []\n",
    "        for inp, out, w, activation in zip(reversed(self._layers_inp), reversed(self._layers_out), reversed(self.weights), reversed(self.activations)):\n",
    "            dactivation = activation(inp, derivative=True)*last_delta\n",
    "            last_delta = np.dot(dactivation, w)\n",
    "            dweights.append(np.dot(dactivation.T, out))\n",
    "            dbiases.append(1.0*dactivation.sum(axis=0, keepdims=True))\n",
    "            \n",
    "        self.weights = [w - self.learning_rate*dw for w, dw in zip(self.weights, reversed(dweights))]\n",
    "        self.biases  = [b - self.learning_rate*db for b, db in zip(self.biases, dbiases[::-1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2) (4, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0]).reshape(-1, 1)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 3), (3, 4), (4, 3), (3, 1)]\n",
      "[(1, 3), (1, 4), (1, 3), (1, 1)]\n",
      "epoch:    0/100 cost: 1.2809434133891928\n",
      "epoch:   10/100 cost: 1.2612415471341634\n",
      "epoch:   20/100 cost: 1.2613130464419606\n",
      "epoch:   30/100 cost: 1.2613135225953633\n",
      "epoch:   40/100 cost: 1.2613135257102452\n",
      "epoch:   50/100 cost: 1.2613135257306203\n",
      "epoch:   60/100 cost: 1.2613135257307535\n",
      "epoch:   70/100 cost: 1.261313525730754\n",
      "epoch:   80/100 cost: 1.261313525730754\n",
      "epoch:   90/100 cost: 1.261313525730754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.10891807],\n",
       "       [ 0.8705073 ],\n",
       "       [-0.46006704],\n",
       "       [ 1.6984778 ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = x.shape[1]\n",
    "nn = NeuralNetwork(layers_size=[D, 3, 4, 3, 1], activations=[relu, relu, relu, linear], cost_func=mse, learning_rate=1e-2)\n",
    "\n",
    "print([w.T.shape for w in nn.weights])\n",
    "print([b.shape for b in nn.biases])\n",
    "nn.fit(x, y)\n",
    "nn.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
