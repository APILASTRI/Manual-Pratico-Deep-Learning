{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Funções de Ativação](#Funções-de-Ativação)\n",
    "\n",
    "[Implementação](#Implementação)\n",
    "\n",
    "[Teste](#Teste)\n",
    "\n",
    "[Referências](#Referências)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de Ativação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(x, derivative=False):\n",
    "    return np.ones_like(x) if derivative else x\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "    if derivative:\n",
    "        y = sigmoid(x)\n",
    "        return y*(1-y)\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "    if derivative:\n",
    "        y = tanh(x)\n",
    "        return 1 - y**2\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return np.where(x <= 0, 0, 1)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return np.where(x <= 0, 0.1, 1)\n",
    "    return np.where(x < 0, 0.1*x, x)\n",
    "\n",
    "def gaussian(x, derivative=False):\n",
    "    if derivative:\n",
    "        return -2*x*np.exp(-x**2)\n",
    "    return np.exp(-x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de custo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mse(y, y_pred, derivative=False):\n",
    "    if derivative:\n",
    "        return np.mean(-(y - y_pred))\n",
    "    return np.mean((y - y_pred)**2)\n",
    "\n",
    "def sigmoid_cross_entropy(y, y_pred, derivative=False):\n",
    "    if derivative:\n",
    "        pass\n",
    "    return -np.mean(y*np.log(y_pred) + (1-y)*np.log(1-y_pred))\n",
    "\n",
    "def softmax_cross_entropy(y, y_pred, derivative=False):\n",
    "    if derivative:\n",
    "        return -( y*(1/y_pred) + (1-y)*(1/(1-y_pred)) )\n",
    "    return -np.mean((y*np.log(y_pred) + (1-y)*np.log(1-y_pred)).sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, layers_size, activations, cost_func, learning_rate=1e-3):\n",
    "        self.layers_size = layers_size\n",
    "        self.activations = activations\n",
    "        self.cost_func = cost_func\n",
    "        self.learning_rate = learning_rate\n",
    "        self._layers_inp = [] # entrada da função de ativação\n",
    "        self._layers_out = []\n",
    "        self.weights = self.__init_weights_and_bias()\n",
    "            \n",
    "    def fit(self, x, y, epochs=100, verbose=10):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.__feedforward(x)\n",
    "            self.__backprop(y, y_pred)\n",
    "            \n",
    "            if epoch % verbose == 0:\n",
    "                cost = self.cost_func(y, y_pred)\n",
    "                print(\"epoch: {0:=4}/{1} cost: {2}\".format(epoch, epochs, cost))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.__feedforward(x)\n",
    "    \n",
    "    def __init_weights_and_bias(self):\n",
    "        weights = []\n",
    "        for inp_layer, out_layer in zip(self.layers_size[:-1], self.layers_size[1:]):\n",
    "            weights.append(np.random.randn(out_layer, inp_layer))\n",
    "        return weights\n",
    "        \n",
    "    def __feedforward(self, x):\n",
    "        self._layers_out.append(x)\n",
    "        for w, activation in zip(self.weights, self.activations):\n",
    "            y = np.dot(self._layers_out[-1], w.T)\n",
    "            self._layers_inp.append(y)\n",
    "            self._layers_out.append(activation(y))\n",
    "        return self._layers_out[-1]\n",
    "    \n",
    "    def __backprop(self, y, y_pred):\n",
    "        self._layers_out.pop()\n",
    "        \n",
    "        dout = self.cost_func(y, y_pred, derivative=True)\n",
    "        last_delta = dout\n",
    "        dweights = []\n",
    "        for inp, out, w, activation in zip(reversed(self._layers_inp), reversed(self._layers_out), reversed(self.weights), reversed(self.activations)):\n",
    "            dactivation = activation(inp, derivative=True)*last_delta\n",
    "            last_delta = np.dot(dactivation, w)\n",
    "            dweights.append(np.dot(dactivation.T, out))\n",
    "            \n",
    "        for i, dw in zip(reversed(range(len(self.weights))), dweights):\n",
    "            self.weights[i] = self.weights[i] - self.learning_rate*dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2) (4, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0]).reshape(-1, 1)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 3), (3, 4), (4, 3), (3, 1)]\n",
      "epoch:    0/100 cost: 0.46492462674886725\n",
      "epoch:   10/100 cost: 0.44967891522200815\n",
      "epoch:   20/100 cost: 0.4321091782287205\n",
      "epoch:   30/100 cost: 0.4119916082915246\n",
      "epoch:   40/100 cost: 0.3902213475188104\n",
      "epoch:   50/100 cost: 0.3697626865216482\n",
      "epoch:   60/100 cost: 0.3562289335772501\n",
      "epoch:   70/100 cost: 0.35494434614031584\n",
      "epoch:   80/100 cost: 0.36801867168497454\n",
      "epoch:   90/100 cost: 0.39135899778297\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(layers_size=[2, 3, 4, 3, 1], activations=[relu, relu, relu, linear], cost_func=mse, learning_rate=1e-2)\n",
    "\n",
    "print([w.T.shape for w in nn.weights])\n",
    "nn.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
