{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Funções de Ativação](#Funções-de-Ativação)\n",
    "\n",
    "[Implementação](#Implementação)\n",
    "\n",
    "[Teste](#Teste)\n",
    "\n",
    "[Referências](#Referências)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de Ativação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(x, derivative=False):\n",
    "    return np.ones_like(x) if derivative else x\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "    if derivative:\n",
    "        y = sigmoid(x)\n",
    "        return y*(1-y)\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "    if derivative:\n",
    "        y = tanh(x)\n",
    "        return 1 - y**2\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return np.where(x <= 0, 0, 1)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return np.where(x <= 0, 0.1, 1)\n",
    "    return np.where(x < 0, 0.1*x, x)\n",
    "\n",
    "def gaussian(x, derivative=False):\n",
    "    if derivative:\n",
    "        return -2*x*np.exp(-x**2)\n",
    "    return np.exp(-x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de custo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mse(y, y_pred, derivative=False):\n",
    "    if derivative:\n",
    "        return -(y - y_pred)\n",
    "    return np.mean((y - y_pred)**2)\n",
    "\n",
    "def sigmoid_cross_entropy(y, y_pred, derivative=False):\n",
    "    if derivative:\n",
    "        return -(y - y_pred)\n",
    "    return -np.mean(y*np.log(y_pred) + (1-y)*np.log(1-y_pred))\n",
    "\n",
    "def softmax_cross_entropy(y, y_pred, derivative=False):\n",
    "    if derivative:\n",
    "        return -( y*(1/y_pred) + (1-y)*(1/(1-y_pred)) )\n",
    "    return -np.mean((y*np.log(y_pred) + (1-y)*np.log(1-y_pred)).sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, layers_size, activations, cost_func, learning_rate=1e-3):\n",
    "        self.layers_size = layers_size\n",
    "        self.activations = activations\n",
    "        self.cost_func = cost_func\n",
    "        self.learning_rate = learning_rate\n",
    "        self._activ_inp = []\n",
    "        self._activ_out = []\n",
    "        self.weights = [np.random.randn(out, inp) for inp, out in zip(self.layers_size[:-1], self.layers_size[1:])]\n",
    "        self.biases = [np.random.randn(1, out) for out in self.layers_size[1:]]\n",
    "            \n",
    "    def fit(self, x, y, epochs=100, verbose=10):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.__feedforward(x)\n",
    "            self.__backprop(y, y_pred)\n",
    "            \n",
    "            if epoch % verbose == 0:\n",
    "                cost = self.cost_func(y, y_pred)\n",
    "                print(\"epoch: {0:=4}/{1} cost: {2:.8f}\".format(epoch, epochs, cost))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.__feedforward(x)\n",
    "    \n",
    "    def __feedforward(self, x):\n",
    "        self._activ_inp, self._activ_out = [], []\n",
    "        self._activ_out.append(x)\n",
    "        for w, b, activation in zip(self.weights, self.biases, self.activations):\n",
    "            y = np.dot(self._activ_out[-1], w.T) + b\n",
    "            self._activ_inp.append(y)\n",
    "            self._activ_out.append(activation(y))\n",
    "        return self._activ_out[-1]\n",
    "    \n",
    "    def __backprop(self, y, y_pred):\n",
    "        self._activ_out.pop()\n",
    "        \n",
    "        last_delta = self.cost_func(y, y_pred, derivative=True)\n",
    "        dweights, dbiases = [], []\n",
    "        for inp, out, w, activation in zip(reversed(self._activ_inp), reversed(self._activ_out), reversed(self.weights), reversed(self.activations)):\n",
    "            dactivation = activation(inp, derivative=True)*last_delta\n",
    "            last_delta = np.dot(dactivation, w)\n",
    "            dweights.append(np.dot(dactivation.T, out))\n",
    "            dbiases.append(1.0*dactivation.sum(axis=0, keepdims=True))\n",
    "        \n",
    "        self.weights = [w - self.learning_rate*dw for w, dw in zip(self.weights, reversed(dweights))]\n",
    "        self.biases  = [b - self.learning_rate*db for b, db in zip(self.biases, reversed(dbiases))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    0/1 cost: 0.29837111\n",
      "[[ 0.14978072  0.19956143]\n",
      " [ 0.24975114  0.29950229]]\n",
      "[[ 0.35891648  0.40866619]\n",
      " [ 0.51130127  0.56137012]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.05, 0.10]])\n",
    "y = np.array([[0.01, 0.99]])\n",
    "\n",
    "D = x.shape[1]\n",
    "\n",
    "nn = NeuralNetwork(layers_size=[D, 2, 2], activations=[sigmoid, sigmoid], cost_func=mse, learning_rate=0.5)\n",
    "\n",
    "w1 = np.array([[0.15, 0.20], [0.25, 0.30]])\n",
    "b1 = np.array([[0.35]])\n",
    "w2 = np.array([[0.40, 0.45], [0.50, 0.55]])\n",
    "b2 = np.array([[0.60]])\n",
    "\n",
    "nn.weights = [w1, w2]\n",
    "nn.biases = [b1, b2]\n",
    "\n",
    "nn.fit(x, y, epochs=1, verbose=1)\n",
    "for w in nn.weights:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porta AND/OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2) (4, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 0, 0, 1]).reshape(-1, 1)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    0/5000 cost: 0.22835709\n",
      "epoch: 1000/5000 cost: 0.00264449\n",
      "epoch: 2000/5000 cost: 0.00124376\n",
      "epoch: 3000/5000 cost: 0.00080595\n",
      "epoch: 4000/5000 cost: 0.00059416\n",
      "[array([[ 7.28015246,  7.28015246]])] [array([[-11.00775858]])]\n",
      "[[  1.65723460e-05]\n",
      " [  2.34855061e-02]\n",
      " [  2.34855061e-02]\n",
      " [  9.72146458e-01]]\n"
     ]
    }
   ],
   "source": [
    "D = x.shape[1]\n",
    "nn = NeuralNetwork(layers_size=[D, 1], activations=[sigmoid], cost_func=mse, learning_rate=1.0)\n",
    "\n",
    "nn.fit(x, y, epochs=5000, verbose=1000)\n",
    "print(nn.weights, nn.biases)\n",
    "print(nn.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porta XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2) (4, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0]).reshape(-1, 1)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    0/5000 cost: 0.43019780\n",
      "epoch: 1000/5000 cost: 0.00218392\n",
      "epoch: 2000/5000 cost: 0.00083212\n",
      "epoch: 3000/5000 cost: 0.00050598\n",
      "epoch: 4000/5000 cost: 0.00036157\n",
      "[[ 0.01842186]\n",
      " [ 0.9839851 ]\n",
      " [ 0.98418141]\n",
      " [ 0.01661788]]\n"
     ]
    }
   ],
   "source": [
    "D = x.shape[1]\n",
    "nn = NeuralNetwork(layers_size=[D, 2, 1], activations=[sigmoid, sigmoid], cost_func=mse, learning_rate=1.0)\n",
    "\n",
    "nn.fit(x, y, epochs=5000, verbose=1000)\n",
    "print(nn.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
