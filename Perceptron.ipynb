{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No notebook anterior, nós aprendemos intuitivamente como o perceptron aprende. De maneira geral, nós vamos atualizando os pesos e o bias sempre buscando diminuir uma função de custo. Nesse notebook, nós vamos ver como esse aprendizado realmente acontence, tanto na teoria quanto na prática. Também utilizaremos o Perceptron para resolver problemas de regressão, classificação e regressão logística.\n",
    "\n",
    "__Objetivos__:\n",
    "\n",
    "- Implementar o perceptron e seu modelo de aprendizado em python puro, Numpy, Keras e Tensorflow\n",
    "- Utilizar o perceptron para regressão e classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T16:28:59.547550Z",
     "start_time": "2017-09-14T16:28:19.688657Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O tipo mais básico de Rede Neural Artificial é formada por apenas um neurônio, o __Perceptron__. O Perceptron nada mais é que um  classificador/regressor linear responsável por mapear uma ou mais entradas em uma saída desejada. Além disso, o método de aprendizagem do Perceptron é adaptativo, ou seja, os melhores parâmetros são aprendidos utilizando algum método de minimização conhecido. Em geral, esse método é o _Gradiente Descendente_, mas outros métodos de minimização podem ser utilizados.\n",
    "\n",
    "O perceptron é formado por:\n",
    "\n",
    "<img src='images/perceptron.png' width='350'>\n",
    "\n",
    "- __entradas__ $x_1 ... x_D$: representam os atributos dos seus dados com dimensionalidade D. O Perceptron aceita qualquer tamanho de entrada, porém a saída é sempre apenas um valor.\n",
    "- __junção aditiva__ $\\sum$: também chamada de _função agregadora_, nada mais é que a soma ponderada das entradas com os __pesos__ ($w_1 ... w_D)$. Em geral, também é alimentada por um __bias__ $b$, responsável por deslocar o resultado do somatório. A junção aditiva é descrita pela seguinte fórmula:\n",
    "\n",
    "$$\\sum_i^D{w_ix_i} + b$$\n",
    "\n",
    "- __função de ativação__ $f$: inicialmente, o perceptron foi projetado para utilizar a função de ativação linear, ou seja, o resultado da junção aditiva representava a saída do Perceptron.\n",
    "\n",
    "Logo, o Perceptron é representado pela seguinte fórmula matemática:\n",
    "\n",
    "$$y_{pred} = f(\\sum_i^D{w_ix_i} + b)$$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $D$: representa a dimensionalidade das amostras, ou seja, a quantidade de atributos de cada amostra.\n",
    "- $x$: representa os atributos dos nossos dados que servem de entrada para o Perceptron.\n",
    "- $w$: representam os __pesos sinápticos__ que ponderam as entradas.\n",
    "- $b$: representa o __bias__, responsável por deslocar a fronteira de decisão além da origem e não depende de nenhum valor de entrada. Repare que o bias encontra-se fora do somatório.\n",
    "- $f$: função de ativação.\n",
    "- $y_{pred}$: representa a saída do Perceptron (o valor predito).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como o Perceptron Aprende?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se você já leu sobre o Perceptron antes, você provavelmente já viu que o modo como o perceptron aprende - isto é, a forma como os pesos são atualizados -, dá-se pela seguite fórmula:\n",
    "\n",
    "$$w_i = w_i + \\lambda(y_i - y_{pred})x_i$$\n",
    "\n",
    "Onde $\\lambda$ é a __taxa de aprendizagem__.\n",
    "\n",
    "Mas você já imaginou da onde vem essa fórmula? Em primeiro lugar, o método de atulização dos pesos é baseado no __Gradiente Descendente__. Sendo $\\overrightarrow{w} = \\{w_1, w_2, ..., w_D\\}$, a atualização dos pesos é dados por:\n",
    "\n",
    "$$\\overrightarrow{w} = \\overrightarrow{w} + \\Delta{\\overrightarrow{w}}$$\n",
    "\n",
    "em que:\n",
    "\n",
    "$$\\Delta{\\overrightarrow{w}} = \\lambda\\nabla E(\\overrightarrow{w})$$\n",
    "\n",
    "Sendo $\\nabla E(\\overrightarrow{w})$ o gradiente de uma função que depende de $\\overrightarrow{w}$ e que queremos minimizar.\n",
    "\n",
    "Agora, imagine que estamos utilizando um Perceptron para regressão, cuja função de custo geralmente é:\n",
    "\n",
    "$$J(w) = \\frac{1}{2}\\sum_{i}^N (y_i - y_{pred_i})^2$$\n",
    "\n",
    "Onde $N$ é a quantidade de amostras nos dados, e as demais variáveis representam as mesmas vistas anteriormente. Repare que a função de custo é quase uma _Mean Squared Error (MSE)_, só que ao invés de dividir por $N$, está se dividindo por 2 o resultado do somatório. O por quê disso será entendido mais a frente na demonstração.\n",
    "\n",
    "Queremos encontrar o vetor $\\overrightarrow{w}$ que minimiza a função $J$. Assim, temos:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_i} = \\frac{\\partial}{\\partial w_i}\\frac{1}{2}\\sum_i^N (y_i - y_{pred_i})^2$$\n",
    "\n",
    "Como a derivada do somatório é igual ao somatório das derivadas:\n",
    "\n",
    "$$= \\frac{1}{2}\\sum_i^N \\frac{\\partial}{\\partial w_i}(y_i - y_{pred_i})^2$$\n",
    "\n",
    "Aplicando a regra da cadeia:\n",
    "\n",
    "$$= \\sum_i^N (y_i - y_{pred_i})\\frac{\\partial}{\\partial w_i}(y_i - y_{pred_i})$$\n",
    "\n",
    "Repare que, quando derivamos $(y_i - y_{pred_i})^2$, o expoente 2, ao sair do somatório, foi multiplicado por $\\frac{1}{2}$, tornando-o 1. Isso é o que os matemáticos denominam de \"conveniência matemática\". \n",
    "\n",
    "Como $y_{pred_i} = w_i*x_i + b$ é uma função que depende de $w$, e sua derivada em relação a $w_i$ é apenas $x_i$, temos que:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_i} = \\sum_i^N (y_i - y_{pred_i})(-x_i)$$\n",
    "$$\\frac{\\partial J}{\\partial w_i} = -\\sum_i^N (y_i - y_{pred_i})x_i$$\n",
    "\n",
    "Na forma vetorizada, essa equação torna-se:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\overrightarrow{w}} = -(\\overrightarrow{y} - \\overrightarrow{y}_{pred})\\overrightarrow{x}$$\n",
    "\n",
    "Como o gradiente nos dá a direção que a função cresce, e queremos minimizar a função, podemos remover o sinal de menos em frente ao resultado. Logo, chegamos que a atualização dos pesos pelo perceptron é dada por $w_i = w_i + \\lambda(y_i - y_{pred})x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regressão "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T16:28:59.578563Z",
     "start_time": "2017-09-14T16:28:59.552044Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Altura</th>\n",
       "      <th>Peso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>187</td>\n",
       "      <td>109.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>177</td>\n",
       "      <td>91.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180</td>\n",
       "      <td>88.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177</td>\n",
       "      <td>89.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>177</td>\n",
       "      <td>92.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>183</td>\n",
       "      <td>94.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>178</td>\n",
       "      <td>83.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>177</td>\n",
       "      <td>85.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>183</td>\n",
       "      <td>92.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>188</td>\n",
       "      <td>108.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Altura    Peso\n",
       "0     187  109.72\n",
       "1     177   91.09\n",
       "2     180   88.93\n",
       "3     177   89.39\n",
       "4     177   92.02\n",
       "5     183   94.70\n",
       "6     178   83.57\n",
       "7     177   85.19\n",
       "8     183   92.96\n",
       "9     188  108.21"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/medidas.csv')\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T16:28:59.982348Z",
     "start_time": "2017-09-14T16:28:59.583066Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x13697028518>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13696ea53c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = df.Altura.values\n",
    "y = df.Peso.values\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('Altura')\n",
    "plt.ylabel('Peso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T16:28:59.994858Z",
     "start_time": "2017-09-14T16:28:59.986852Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (100,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T16:29:00.215014Z",
     "start_time": "2017-09-14T16:28:59.999361Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "x = x.reshape(-1, 1)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__Exercício__: tentar estimar as learning_rates de __w__ e __b__. Elas são diferentes por que nossos dados não estão na mesma escala!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T16:29:22.792567Z",
     "start_time": "2017-09-14T16:29:00.219017Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: [ 4394.82509891]\n",
      "step 1000: [-7.98977015]\n",
      "step 2000: [-3.35286654]\n",
      "step 3000: [-1.40701345]\n",
      "step 4000: [-0.59044606]\n",
      "step 5000: [-0.2477777]\n",
      "step 6000: [-0.10397865]\n",
      "step 7000: [-0.04363412]\n",
      "step 8000: [-0.01831084]\n",
      "step 9000: [-0.00768405]\n",
      "[ 1.36976658] [-157.87590789]\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "D = x.shape[1]\n",
    "w = 2*random() - 1 # [1, 1]\n",
    "b = 2*random() - 1 # [1, 1]\n",
    "\n",
    "for step in range(10000):\n",
    "    sum_error = 0\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        y_pred = w*x_i + b\n",
    "        error = y_i - y_pred\n",
    "        w = w + 1e-7*error*x_i\n",
    "        b = b + 1e-2*error\n",
    "        sum_error += error\n",
    "        \n",
    "    if step%1000 == 0:\n",
    "        print('step {0}: {1}'.format(step, sum_error))\n",
    "\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Numpy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Versão não-vetorizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T16:29:49.656665Z",
     "start_time": "2017-09-14T16:29:22.797570Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: [ 12490.5281522]\n",
      "step 1000: [-12.01982349]\n",
      "step 2000: [-5.044058]\n",
      "step 3000: [-2.11671337]\n",
      "step 4000: [-0.88826803]\n",
      "step 5000: [-0.37275718]\n",
      "step 6000: [-0.15642566]\n",
      "step 7000: [-0.06564324]\n",
      "step 8000: [-0.02754685]\n",
      "step 9000: [-0.0115599]\n",
      "[[ 1.3696544]] [-157.85716899]\n"
     ]
    }
   ],
   "source": [
    "D = x.shape[1]\n",
    "w = 2*np.random.random((1, D)) - 1 # [1, 1]\n",
    "b = 2*np.random.random() - 1       # [1, 1]\n",
    "\n",
    "for step in range(10000):\n",
    "    sum_error = 0\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        # y_pred = w*x_i + b\n",
    "        # y_pred = np.dot(w, x_i) + b\n",
    "        y_pred = np.dot(w, x_i.T) + b # [1,1] x [1,1] + [1,1] = [1,1]\n",
    "        error = y_i - y_pred\n",
    "        w = w + 1e-7*error*x_i\n",
    "        b = b + 1e-2*error\n",
    "        sum_error += error\n",
    "        \n",
    "    if step%1000 == 0:\n",
    "        print('step {0}: {1}'.format(step, sum_error))\n",
    "\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Versão vetorizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T16:29:50.081967Z",
     "start_time": "2017-09-14T16:29:49.662171Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 4357.77158\n",
      "step 1000: -6.99365\n",
      "step 2000: -2.92017\n",
      "step 3000: -1.21931\n",
      "step 4000: -0.50912\n",
      "step 5000: -0.21258\n",
      "step 6000: -0.08876\n",
      "step 7000: -0.03706\n",
      "step 8000: -0.01548\n",
      "step 9000: -0.00646\n",
      "[[ 1.3713131]] -157.441273367\n"
     ]
    }
   ],
   "source": [
    "D = x.shape[1]\n",
    "w = 2*np.random.random((1, D))-1 # [1, 1]\n",
    "b = 2*np.random.random()-1       # [1, 1]\n",
    "\n",
    "for step in range(10000):\n",
    "    y_pred = np.dot(w, x.T) + b # [1,1]x[1,100] + [1,1] = [1,100]\n",
    "    error = y - y_pred # [1,100] + [100] = [1,100]\n",
    "    w = w + 1e-7*np.dot(error, x) # [1,1] + [1,100]x[100,1] = [1,1]\n",
    "    b = b + 1e-2*np.sum(error)\n",
    "    \n",
    "    if step%1000 == 0:\n",
    "        print('step {0}: {1:.5f}'.format(step, np.sum(error)))\n",
    "    \n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Versão vetorizada com pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T16:29:50.651872Z",
     "start_time": "2017-09-14T16:29:50.087473Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minmax = MinMaxScaler()\n",
    "x_scale = minmax.fit_transform(x.astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T16:29:50.905058Z",
     "start_time": "2017-09-14T16:29:50.656375Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 7361.06145\n",
      "step 1000: -1.65280\n",
      "step 2000: -0.03887\n",
      "step 3000: -0.00091\n",
      "step 4000: -0.00002\n",
      "[[ 67.20329507]] 41.3947153375\n"
     ]
    }
   ],
   "source": [
    "D = x.shape[1]\n",
    "w = 2*np.random.random((1, D))-1 # [1, 1]\n",
    "b = 2*np.random.random()-1       # [1, 1]\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "for step in range(5000):\n",
    "    y_pred = np.dot(w, x_scale.T) + b # [1,1]x[1,100] + [1,1] = [1,100]\n",
    "    error = y - y_pred # [1,100] + [100] = [1,100]\n",
    "    w = w + learning_rate*np.dot(error, x_scale) # [1,1] + [1,100]x[100,1] = [1,1]\n",
    "    b = b + learning_rate*np.sum(error)\n",
    "    \n",
    "    if step%1000 == 0:\n",
    "        print('step {0}: {1:.5f}'.format(step, np.sum(error)))\n",
    "    \n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T16:29:58.979793Z",
     "start_time": "2017-09-14T16:29:50.910557Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.1159591675\n",
      "[[ 66.36580658]] [ 41.82068634]\n"
     ]
    }
   ],
   "source": [
    "D = x_scale.shape[1]\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model = Sequential()\n",
    "model.add(Dense(units=1, activation='linear', input_shape=(D,)))\n",
    "model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "model.fit(x_scale, y, batch_size=x_scale.shape[0], epochs=5000, verbose=0)\n",
    "error = model.evaluate(x_scale, y, batch_size=x_scale.shape[0], verbose=0)\n",
    "w, b = model.get_weights()\n",
    "print(error)\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## TensorFlow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T16:29:59.244481Z",
     "start_time": "2017-09-14T16:29:58.986299Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D = x_scale.shape[1]\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_x = tf.constant(x_scale)\n",
    "    tf_y = tf.constant(y.reshape(1,-1))\n",
    "    \n",
    "    tf_w = tf.Variable(tf.random_uniform(shape=(1, D), minval=-1, maxval=1, dtype=tf.float64))\n",
    "    tf_b = tf.Variable(tf.random_uniform(shape=(1, D), minval=-1, maxval=1, dtype=tf.float64))\n",
    "    \n",
    "    y_pred = tf.add(tf.matmul(tf_w, tf.transpose(tf_x)), tf_b)\n",
    "    error = tf.subtract(tf_y, y_pred)\n",
    "    loss = tf.reduce_mean(tf.square(error))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T16:30:02.136332Z",
     "start_time": "2017-09-14T16:29:59.248985Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 5551.060918211068\n",
      "step 1000: 39.97749330185879\n",
      "step 2000: 29.190119553682862\n",
      "step 3000: 26.777769899016686\n",
      "step 4000: 26.238303019290534\n",
      "26.1177156634\n",
      "[[ 66.34448384]] [[ 41.83153253]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(5000):\n",
    "        _, error = sess.run([optimizer, loss], feed_dict={tf_x:x_scale, tf_y:y.reshape(1,-1)})\n",
    "\n",
    "        if step%1000 == 0:\n",
    "            print('step {0}: {1}'.format(step, error))\n",
    "    \n",
    "    print(error)\n",
    "    print(tf_w.eval(), tf_b.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
